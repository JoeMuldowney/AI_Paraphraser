{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03af3bc7-8e7e-4fdc-91dc-86effd50dbf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer, BartForConditionalGeneration, BartTokenizer\n",
    "from datasets import load_dataset, load_from_disk, Dataset\n",
    "\n",
    "tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-base\")\n",
    "model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "224d1b72-0da0-4f04-9560-9b22be9aee8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_train = pd.read_csv('datasets/training.csv')\n",
    "df_val = pd.read_csv('datasets/validating.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "377935b7-a11f-4610-bff2-dfec3f55fb51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop any rows with missing data\n",
    "cleaned_training_df = df_train.dropna()\n",
    "cleaned_validate_df = df_val.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7f8b6d8-1883-46d1-a932-5233dead4f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 48\n",
    "max_input = 1024\n",
    "max_target = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97d3ab60-cab6-451d-b70c-dd1230f32408",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle and sample from training & validation sets\n",
    "train_data = cleaned_training_df.sample(n=15000, random_state=56)\n",
    "val_data = cleaned_validate_df.sample(n=3000, random_state=56)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66a4e7be-e299-489a-8d0b-181d2476a2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset indexes\n",
    "train_data.reset_index(drop=True, inplace=True)\n",
    "val_data.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2768baa-f681-481b-b18b-0f2bd51f10f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to Hugging Face dataset from pandas\n",
    "train_dataset = Dataset.from_pandas(train_data)\n",
    "val_dataset = Dataset.from_pandas(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "775b2bcc-9ddb-40c2-9767-baa22045b4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(batch_data_to_process):\n",
    "    \n",
    "    # Extract articles from the dataset\n",
    "    inputs = [article for article in batch_data_to_process['article']]\n",
    "    targets = [summary for summary in batch_data_to_process['highlights']]\n",
    "    \n",
    "    # Tokenize articles\n",
    "    model_inputs = tokenizer(\n",
    "        inputs, \n",
    "        max_length=max_input, \n",
    "        padding='max_length', \n",
    "        truncation=True\n",
    "    )\n",
    "\n",
    "     # Tokenize summaries\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(targets, \n",
    "            max_length=max_target, \n",
    "            padding='max_length', \n",
    "            truncation=True\n",
    "    )   \n",
    "    # Set tokenized summaries as labels\n",
    "    model_inputs['labels'] = labels['input_ids']\n",
    "    \n",
    "    # Return preprocessed data\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2cbd19a3-a40d-42f8-bfbb-8fc7e3b5b35f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cd4bbfce76648a592ab246ceaa953c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/15000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\muldo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:3980: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f2af48b4ede4d20bab0f4da4d874cfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Apply preprocessing to train and validation sets\n",
    "tokenized_train = train_dataset.map(preprocess_data, batched=True)\n",
    "tokenized_validation = val_dataset.map(preprocess_data, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c6ac9a-c0ef-4a36-9df9-44ef299389a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\muldo\\AppData\\Local\\Temp\\ipykernel_3772\\3482062024.py:15: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n"
     ]
    }
   ],
   "source": [
    "args = Seq2SeqTrainingArguments(\n",
    "    'epoch4_lrate2e_b48_s15000v3000', #save directory\n",
    "    eval_strategy='steps',\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=4,\n",
    "    warmup_steps=500,\n",
    "    predict_with_generate=True,\n",
    "    eval_steps=500,\n",
    "    logging_steps=500,\n",
    "    dataloader_num_workers=4,  # Use 4 CPU threads for loading data\n",
    "    save_total_limit=1,  \n",
    "    fp16=False #available only with CUDA\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model, \n",
    "    args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_validation,   \n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1972e037-f6ae-43c3-a8a9-518932629a97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('epoch4_lrate1e_b48_s15000v3000\\\\tokenizer_config.json',\n",
       " 'epoch4_lrate1e_b48_s15000v3000\\\\special_tokens_map.json',\n",
       " 'epoch4_lrate1e_b48_s15000v3000\\\\vocab.json',\n",
       " 'epoch4_lrate1e_b48_s15000v3000\\\\merges.txt',\n",
       " 'epoch4_lrate1e_b48_s15000v3000\\\\added_tokens.json')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained('epoch4_lrate2e_b48_s15000v3000')\n",
    "tokenizer.save_pretrained('epoch4_lrate2e_b48_s15000v3000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa23eb37-120c-48c2-9a7e-14ea1c39827e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
